{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "\n",
    "[SocioPatterns.org/Datasets](http://www.sociopatterns.org/datasets/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "base_dir = '../../../SocioPatterns'#os.path.expanduser(r'~\\Desktop\\MScThesis\\SocioPatterns')\n",
    "work_contacts_dir = os.path.join(base_dir, r'Contacts in a workplace')\n",
    "primary_school_dir = os.path.join(base_dir, r'Primary school temporal')\n",
    "hospital_ward_dir = os.path.join(base_dir, r'Hospital ward')\n",
    "out_dir = os.path.expanduser('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# packages in environment at C:\\Users\\mstud\\Anaconda3\\envs\\pathpy2:\n",
      "#\n",
      "# Name                    Version                   Build  Channel\n",
      "pathpy2                   2.2.0                    pypi_0    pypi\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# this workbook needs pathpy2 installed\n",
    "%conda list pathpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathpy as pp\n",
    "import numpy as np\n",
    "import scipy.sparse.linalg as sla\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_transitions(hon, include_subpaths:bool=True):\n",
    "    \"iterate over transition matrix of HigherOrderNetwork\"\n",
    "    mat = hon.transition_matrix(include_subpaths)\n",
    "    if hon.order == 1:\n",
    "        nodes = list((n,) for n in hon.nodes.keys())\n",
    "    else:\n",
    "        nodes = list(tuple(n.split(hon.separator)) for n in hon.nodes.keys())\n",
    "    for r in range(mat.shape[0]):\n",
    "        for ind in range(mat.indptr[r], mat.indptr[r+1]):\n",
    "            yield nodes[mat.indices[ind]], nodes[r][-1], mat.data[ind] # yield (start, next_node, prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# orig: Calc_q.ipynb\n",
    "def calc_q(net, C, weighted=False, **kwargs):\n",
    "    nodes = list(net.nodes.keys())\n",
    "    mat = net.adjacency_matrix(weighted=weighted, **kwargs) # scipy.sparse.csc.csc_matrix\n",
    "    mat_sum = 0\n",
    "    mat_sum_by_row_C = defaultdict(float)\n",
    "    mat_sum_by_col_C = defaultdict(float)\n",
    "    q = 0\n",
    "    for c in range(mat.shape[1]):\n",
    "        c_C = C[nodes[c]] # class of current column's node\n",
    "        for ind in range(mat.indptr[c], mat.indptr[c+1]):\n",
    "            r = mat.indices[ind]\n",
    "            v = mat.data[ind]\n",
    "            # assert mat[r,c]==v\n",
    "            r_C = C[nodes[r]] # class of current rows's node\n",
    "            mat_sum += v\n",
    "            mat_sum_by_row_C[r_C] += v\n",
    "            mat_sum_by_col_C[c_C] += v\n",
    "            if c_C == r_C:\n",
    "                q += v\n",
    "    q_exp = sum( v*mat_sum_by_col_C[c] for c,v in mat_sum_by_row_C.items() ) / (mat_sum**2)\n",
    "    q = q/mat_sum - q_exp\n",
    "    q_max = 1 - q_exp\n",
    "    return (q, q_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_rules(filename, paths, config, metadata, max_order:int=1, replace_space=False, include_subpaths=True):\n",
    "    \"Export the transition probabilities (rules) up to the estimated order\"\n",
    "    mog = pp.MultiOrderModel(paths, max_order)\n",
    "    estimated_order = mog.estimate_order(paths)\n",
    "    print('Estimated order:', estimated_order)\n",
    "    stats = dict()\n",
    "    with open(filename,'w') as f:\n",
    "        for order in range(1,estimated_order+1):\n",
    "            hon = mog.layers[order]\n",
    "            if order == 1:\n",
    "                print('Exporting stationary distribution')\n",
    "                probs = hon.transition_matrix(include_subpaths)\n",
    "                _,ev = sla.eigs(probs, k=1, which='LM')\n",
    "                ev = np.abs(ev).flatten()\n",
    "                ev_sum = ev.sum()\n",
    "                stat_dist = {n:v/ev_sum for n,v in zip(hon.nodes.keys(), ev)}\n",
    "                for n,v in sorted(list(stat_dist.items())):\n",
    "                    f.write('=> %s %r\\n' % (n, v))\n",
    "            print('Exporting rules for order',order)\n",
    "            #for start,next_node,prob in iter_transitions(hon):\n",
    "            for start,next_node,prob in sorted(list(iter_transitions(hon, include_subpaths))):\n",
    "                if replace_space:\n",
    "                    next_node = next_node.replace(' ','_')\n",
    "                    start = tuple( n.replace(' ','_') for n in start)\n",
    "                for n in start:\n",
    "                    f.write(n + ' ')\n",
    "                f.write('=>')\n",
    "                f.write(' %s %r\\n' % (next_node,prob))\n",
    "            # calc q, q_max\n",
    "            node2cat = { n:metadata[n.split(',')[-1]] for n in hon.nodes } # last node is relevant for category\n",
    "            q, q_max = calc_q(hon, node2cat, weighted=True, include_subpaths=include_subpaths)\n",
    "            print(f'order {hon.order}: q={q}, q_max={q_max}, q/q_max={q/q_max}')\n",
    "            stats.update({ f'q[{hon.order}]': q, f'q_max[{hon.order}]': q_max, f'q/q_max[{hon.order}]': q/q_max })\n",
    "    config_filename = os.path.splitext(filename)[0]+'.config'\n",
    "    with open(config_filename,'w') as f:\n",
    "        f.write('code\\t%s\\n' % os.path.join(os.getcwd(), 'SocioPatterns.ipynb'))\n",
    "        for k,v in config.items():\n",
    "            f.write('%s\\t%s\\n' % (k,v))\n",
    "        delta_min,delta_sec = config['delta'] // 3, config['delta'] % 3 *20\n",
    "        delta_time = f'{delta_min}:{delta_sec:02d}'\n",
    "        delta_text = '%d min' % delta_min if delta_sec==0 else '%d sec' % delta_sec if delta_min==0 else delta_time\n",
    "        config_loc = dict(max_order=max_order, estimated_order=estimated_order, replace_space=replace_space, include_subpaths=include_subpaths,\n",
    "                         delta_time=delta_time, delta_text=delta_text)\n",
    "        for k,v in config_loc.items():\n",
    "            f.write('%s\\t%s\\n' % (k,v))\n",
    "        for k,v in stats.items():\n",
    "            f.write('%s\\t%s\\n' % (k,v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contacts in workplace\n",
    "delta=90 finished in less than 6 hrs;\n",
    "delta=180 did not finish within 24 hrs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes:\t\t\t92\n",
      "Time-stamped links:\t19654\n",
      "Links/Nodes:\t\t213.6304347826087\n",
      "Observation period:\t[1441, 50822]\n",
      "Observation length:\t 49381 \n",
      "Time stamps:\t\t 7104 \n",
      "Avg. inter-event dt:\t 6.952132901590877\n",
      "Min/Max inter-event dt:\t 1/11134\n"
     ]
    }
   ],
   "source": [
    "filename = os.path.join(work_contacts_dir, 'tij_InVS.dat')\n",
    "c_workplace=dict(source=filename)\n",
    "t_workplace = pp.TemporalNetwork()\n",
    "with open(filename,'r') as f:\n",
    "    for line in f:\n",
    "        t,i,j = line.split()\n",
    "        t_workplace.add_edge(i, j, int(t)//20)\n",
    "        t_workplace.add_edge(j, i, int(t)//20)\n",
    "print(t_workplace.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy metadata\n",
    "m_workplace = dict()\n",
    "with open(os.path.join(work_contacts_dir, 'metadata_InVS13.txt'),'r') as f:\n",
    "    with open(os.path.join(out_dir, 'metadata_workplace.csv'),'w') as g:\n",
    "        #g.write(f.read())\n",
    "        for line in f:\n",
    "            i,Ci = line.split()\n",
    "            m_workplace[i]=Ci\n",
    "            g.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-24 23:38:14 [Severity.INFO]\tConstructing time-unfolded DAG ...\n",
      "2020-11-24 23:38:15 [Severity.INFO]\tfinished.\n",
      "Directed Acyclic Graph\n",
      "Nodes:\t\t40046\n",
      "Roots:\t\t6090\n",
      "Leaves:\t\t21223\n",
      "Links:\t\t58962\n",
      "Acyclic:\tNone\n",
      "\n",
      "2020-11-24 23:38:15 [Severity.INFO]\tGenerating causal trees for 6090 root nodes ...\n",
      "2020-11-24 23:38:16 [Severity.INFO]\tAnalyzing tree 609/6090 ...\n",
      "2020-11-24 23:38:16 [Severity.INFO]\tAnalyzing tree 1218/6090 ...\n",
      "2020-11-24 23:38:17 [Severity.INFO]\tAnalyzing tree 1827/6090 ...\n",
      "2020-11-24 23:38:17 [Severity.INFO]\tAnalyzing tree 2436/6090 ...\n",
      "2020-11-24 23:38:17 [Severity.INFO]\tAnalyzing tree 3045/6090 ...\n",
      "2020-11-24 23:38:18 [Severity.INFO]\tAnalyzing tree 3654/6090 ...\n",
      "2020-11-24 23:38:18 [Severity.INFO]\tAnalyzing tree 4263/6090 ...\n",
      "2020-11-24 23:38:23 [Severity.INFO]\tAnalyzing tree 4872/6090 ...\n",
      "2020-11-24 23:38:24 [Severity.INFO]\tAnalyzing tree 5481/6090 ...\n",
      "2020-11-24 23:38:24 [Severity.INFO]\tAnalyzing tree 6090/6090 ...\n",
      "2020-11-24 23:38:24 [Severity.INFO]\tfinished.\n"
     ]
    }
   ],
   "source": [
    "extraction_param = dict(delta=3)\n",
    "#extraction_param = dict(delta=15)\n",
    "#extraction_param = dict(delta=30)\n",
    "#extraction_param = dict(delta=60)\n",
    "#extraction_param = dict(delta=90)\n",
    "c_workplace.update(extraction_param)\n",
    "p_workplace = pp.path_extraction.paths_from_temporal_network_dag(t_workplace, **extraction_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-24 23:38:24 [Severity.INFO]\tGenerating 0-th order layer ...\n",
      "2020-11-24 23:38:24 [Severity.INFO]\tGenerating 1-th order layer ...\n",
      "2020-11-24 23:38:24 [Severity.INFO]\tGenerating 2-th order layer ...\n",
      "2020-11-24 23:38:24 [Severity.INFO]\tGenerating 3-th order layer ...\n",
      "2020-11-24 23:38:25 [Severity.INFO]\tGenerating 4-th order layer ...\n",
      "2020-11-24 23:38:33 [Severity.INFO]\tfinished.\n",
      "2020-11-24 23:38:37 [Severity.INFO]\tLikelihood ratio test for K_opt = 2, x = 99053.05568314757\n",
      "2020-11-24 23:38:37 [Severity.INFO]\tLikelihood ratio test, d_1-d_0 = 29924\n",
      "2020-11-24 23:38:37 [Severity.INFO]\tLikelihood ratio test, p = 0.0\n",
      "2020-11-24 23:38:40 [Severity.INFO]\tLikelihood ratio test for K_opt = 3, x = 10965.6320084834\n",
      "2020-11-24 23:38:40 [Severity.INFO]\tLikelihood ratio test, d_1-d_0 = 591247\n",
      "2020-11-24 23:38:40 [Severity.INFO]\tLikelihood ratio test, p = 1.0\n",
      "2020-11-24 23:38:45 [Severity.INFO]\tLikelihood ratio test for K_opt = 4, x = 5464.151524489163\n",
      "2020-11-24 23:38:45 [Severity.INFO]\tLikelihood ratio test, d_1-d_0 = 11735917\n",
      "2020-11-24 23:38:45 [Severity.INFO]\tLikelihood ratio test, p = 1.0\n",
      "Estimated order: 2\n",
      "Exporting stationary distribution\n",
      "Exporting rules for order 1\n",
      "order 1: q=0.5366648168070582, q_max=0.7222411011294676, q/q_max=0.7430549382578778\n",
      "Exporting rules for order 2\n",
      "order 2: q=0.5355159672440006, q_max=0.7195217097762616, q/q_max=0.744266587050614\n",
      "Wall time: 20.8 s\n"
     ]
    }
   ],
   "source": [
    "%time export_rules(os.path.join(out_dir, 'workplace_%d.csv' % extraction_param['delta']), p_workplace, c_workplace, m_workplace, max_order=4) # estimated_order=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "del p_workplace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primary school\n",
    "Unable to extract paths for delta=2 (on a PC with 16GB RAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes:\t\t\t242\n",
      "Time-stamped links:\t251546\n",
      "Links/Nodes:\t\t1039.4462809917356\n",
      "Observation period:\t[1561, 7406]\n",
      "Observation length:\t 5845 \n",
      "Time stamps:\t\t 3100 \n",
      "Avg. inter-event dt:\t 1.8860922878347854\n",
      "Min/Max inter-event dt:\t 1/2747\n"
     ]
    }
   ],
   "source": [
    "filename = os.path.join(primary_school_dir, 'primaryschool.csv')\n",
    "c_primaryschool = dict(source=filename)\n",
    "t_primaryschool = pp.TemporalNetwork()\n",
    "m_primaryschool = dict() # metadata\n",
    "with open(filename,'r') as f:\n",
    "    for line in f:\n",
    "        t,i,j,Ci,Cj = line.split()\n",
    "        t_primaryschool.add_edge(i, j, int(t)//20)\n",
    "        t_primaryschool.add_edge(j, i, int(t)//20)\n",
    "        m_primaryschool[i]=Ci\n",
    "        m_primaryschool[j]=Cj\n",
    "print(t_primaryschool.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export metadata\n",
    "with open(os.path.join(out_dir, 'metadata_primaryschool.csv'),'w') as g:\n",
    "    for i,c in sorted(list(m_primaryschool.items())):\n",
    "        g.write('%s\\t%s\\n' % (i,c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-24 23:38:54 [Severity.INFO]\tConstructing time-unfolded DAG ...\n",
      "2020-11-24 23:38:58 [Severity.INFO]\tfinished.\n",
      "Directed Acyclic Graph\n",
      "Nodes:\t\t240014\n",
      "Roots:\t\t65218\n",
      "Leaves:\t\t65218\n",
      "Links:\t\t251546\n",
      "Acyclic:\tNone\n",
      "\n",
      "2020-11-24 23:38:58 [Severity.INFO]\tGenerating causal trees for 65218 root nodes ...\n",
      "2020-11-25 02:08:45 [Severity.INFO]\tfinished.\n"
     ]
    }
   ],
   "source": [
    "# delta=1 took 2 hours\n",
    "# delta=2 uses > 50GB RAM\n",
    "extraction_param = dict(delta=1)\n",
    "c_primaryschool.update(extraction_param)\n",
    "p_primaryschool = pp.path_extraction.paths_from_temporal_network_dag(t_primaryschool, **extraction_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-25 02:08:45 [Severity.INFO]\tGenerating 0-th order layer ...\n",
      "2020-11-25 02:08:45 [Severity.INFO]\tGenerating 1-th order layer ...\n",
      "2020-11-25 02:08:50 [Severity.INFO]\tGenerating 2-th order layer ...\n",
      "2020-11-25 02:08:59 [Severity.INFO]\tGenerating 3-th order layer ...\n",
      "2020-11-25 02:09:49 [Severity.INFO]\tfinished.\n",
      "2020-11-25 02:46:00 [Severity.INFO]\tLikelihood ratio test for K_opt = 2, x = 79586108.4486756\n",
      "2020-11-25 02:46:00 [Severity.INFO]\tLikelihood ratio test, d_1-d_0 = 1303716\n",
      "2020-11-25 02:46:00 [Severity.INFO]\tLikelihood ratio test, p = 0.0\n",
      "2020-11-25 03:23:25 [Severity.INFO]\tLikelihood ratio test for K_opt = 3, x = 27330624.334839195\n",
      "2020-11-25 03:23:25 [Severity.INFO]\tLikelihood ratio test, d_1-d_0 = 103903081\n",
      "2020-11-25 03:23:25 [Severity.INFO]\tLikelihood ratio test, p = 1.0\n",
      "Estimated order: 2\n",
      "Exporting stationary distribution\n",
      "Exporting rules for order 1\n",
      "order 1: q=0.44758433082597404, q_max=0.8637274056458426, q/q_max=0.5182009137377062\n",
      "Exporting rules for order 2\n",
      "order 2: q=0.44704898383148034, q_max=0.8629871705213019, q/q_max=0.5180250635260695\n",
      "Wall time: 1h 14min 45s\n"
     ]
    }
   ],
   "source": [
    "%time export_rules(os.path.join(out_dir, 'primaryschool_%d.csv') % extraction_param['delta'], p_primaryschool, c_primaryschool, m_primaryschool, max_order=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "del p_primaryschool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hospital ward\n",
    "delta=3 took 12 hrs to generate paths and 5 hrs to export rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes:\t\t\t75\n",
      "Time-stamped links:\t64848\n",
      "Links/Nodes:\t\t864.64\n",
      "Observation period:\t[7, 17382]\n",
      "Observation length:\t 17375 \n",
      "Time stamps:\t\t 9453 \n",
      "Avg. inter-event dt:\t 1.838235294117647\n",
      "Min/Max inter-event dt:\t 1/1349\n"
     ]
    }
   ],
   "source": [
    "filename = os.path.join(hospital_ward_dir, 'detailed_list_of_contacts_Hospital.dat')\n",
    "c_hospitalward = dict(source=filename)\n",
    "t_hospitalward = pp.TemporalNetwork()\n",
    "m_hospitalward = dict()\n",
    "with open(filename,'r') as f:\n",
    "    for line in f:\n",
    "        t,i,j,Si,Sj = line.split()\n",
    "        t_hospitalward.add_edge(i, j, int(t)//20)\n",
    "        t_hospitalward.add_edge(j, i, int(t)//20)\n",
    "        m_hospitalward[i]=Si\n",
    "        m_hospitalward[j]=Sj\n",
    "print(t_hospitalward.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export metadata\n",
    "with open(os.path.join(out_dir, 'metadata_hospital.csv'),'w') as g:\n",
    "    for i,c in sorted(list(m_hospitalward.items())):\n",
    "        g.write('%s\\t%s\\n' % (i,c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-25 03:23:43 [Severity.INFO]\tConstructing time-unfolded DAG ...\n",
      "2020-11-25 03:23:44 [Severity.INFO]\tfinished.\n",
      "Directed Acyclic Graph\n",
      "Nodes:\t\t66319\n",
      "Roots:\t\t15674\n",
      "Leaves:\t\t15674\n",
      "Links:\t\t64848\n",
      "Acyclic:\tNone\n",
      "\n",
      "2020-11-25 03:23:44 [Severity.INFO]\tGenerating causal trees for 15674 root nodes ...\n",
      "2020-11-25 03:27:03 [Severity.INFO]\tfinished.\n"
     ]
    }
   ],
   "source": [
    "extraction_param = dict(delta=1)\n",
    "#extraction_param = dict(delta=2)\n",
    "#extraction_param = dict(delta=3)\n",
    "c_hospitalward.update(extraction_param)\n",
    "p_hospitalward = pp.path_extraction.paths_from_temporal_network_dag(t_hospitalward, **extraction_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-25 03:27:03 [Severity.INFO]\tGenerating 0-th order layer ...\n",
      "2020-11-25 03:27:03 [Severity.INFO]\tGenerating 1-th order layer ...\n",
      "2020-11-25 03:27:03 [Severity.INFO]\tGenerating 2-th order layer ...\n",
      "2020-11-25 03:27:04 [Severity.INFO]\tGenerating 3-th order layer ...\n",
      "2020-11-25 03:27:07 [Severity.INFO]\tGenerating 4-th order layer ...\n",
      "2020-11-25 03:28:20 [Severity.INFO]\tfinished.\n",
      "2020-11-25 03:29:06 [Severity.INFO]\tLikelihood ratio test for K_opt = 2, x = 1539106.6271252963\n",
      "2020-11-25 03:29:06 [Severity.INFO]\tLikelihood ratio test, d_1-d_0 = 85573\n",
      "2020-11-25 03:29:06 [Severity.INFO]\tLikelihood ratio test, p = 0.0\n",
      "2020-11-25 03:29:51 [Severity.INFO]\tLikelihood ratio test for K_opt = 3, x = 493542.75202822825\n",
      "2020-11-25 03:29:51 [Severity.INFO]\tLikelihood ratio test, d_1-d_0 = 3145815\n",
      "2020-11-25 03:29:51 [Severity.INFO]\tLikelihood ratio test, p = 1.0\n",
      "2020-11-25 03:30:45 [Severity.INFO]\tLikelihood ratio test for K_opt = 4, x = 319552.03792457865\n",
      "2020-11-25 03:30:45 [Severity.INFO]\tLikelihood ratio test, d_1-d_0 = 116407510\n",
      "2020-11-25 03:30:45 [Severity.INFO]\tLikelihood ratio test, p = 1.0\n",
      "Estimated order: 2\n",
      "Exporting stationary distribution\n",
      "Exporting rules for order 1\n",
      "order 1: q=0.1463174159422871, q_max=0.4981693643463372, q/q_max=0.2937101845559582\n",
      "Exporting rules for order 2\n",
      "order 2: q=0.14766272991345886, q_max=0.49217632198532957, q/q_max=0.3000199792582876\n",
      "Wall time: 3min 44s\n"
     ]
    }
   ],
   "source": [
    "%time export_rules(os.path.join(out_dir, 'hospital_%d.csv' % extraction_param['delta']), p_hospitalward, c_hospitalward, m_hospitalward, max_order=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "del p_hospitalward"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
